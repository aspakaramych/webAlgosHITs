{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-02T16:41:41.497366Z",
     "start_time": "2025-04-02T16:41:40.306634Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.font_manager import weight_dict"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T16:53:53.308377Z",
     "start_time": "2025-04-02T16:53:51.300079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"oddrationale/mnist-in-csv\")"
   ],
   "id": "7e7c565511ccdde2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vgrig\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T16:53:58.630244Z",
     "start_time": "2025-04-02T16:53:53.599714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_csv = pd.read_csv(path + \"/mnist_train.csv\")\n",
    "test_csv = pd.read_csv(path + \"/mnist_test.csv\")\n",
    "train_2_csv = pd.read_csv(\"./data/train.csv\")\n",
    "train_csv"
   ],
   "id": "5c654ce416471062",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0          5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1          0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2          4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3          1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4          9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "...      ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...   \n",
       "59995      8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59996      3    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59997      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59998      6    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "59999      8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "       28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0          0      0      0      0      0      0      0      0  \n",
       "1          0      0      0      0      0      0      0      0  \n",
       "2          0      0      0      0      0      0      0      0  \n",
       "3          0      0      0      0      0      0      0      0  \n",
       "4          0      0      0      0      0      0      0      0  \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "59995      0      0      0      0      0      0      0      0  \n",
       "59996      0      0      0      0      0      0      0      0  \n",
       "59997      0      0      0      0      0      0      0      0  \n",
       "59998      0      0      0      0      0      0      0      0  \n",
       "59999      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[60000 rows x 785 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 785 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T16:54:02.680809Z",
     "start_time": "2025-04-02T16:54:01.625175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train = np.array(train_csv.iloc[:, 1:].values)\n",
    "x_2_train = np.array(train_2_csv.iloc[:, 1:].values)\n",
    "y_train = np.array(train_csv.iloc[:, 0].values)\n",
    "y_2_train = np.array(train_2_csv.iloc[:, 0].values)\n",
    "x_train = np.array(np.concatenate([x_train.data, x_2_train.data], axis=0))\n",
    "y_train = np.array(np.concatenate([y_train.data, y_2_train.data], axis=0))\n",
    "print(x_train.shape)\n",
    "x_train = x_train / 255.0\n",
    "x_test = np.array(test_csv.iloc[:, 1:].values)\n",
    "y_test = np.array(test_csv.iloc[:, 0].values)\n",
    "x_test = x_test / 255.0\n",
    "print(x_test.shape)"
   ],
   "id": "5f604826a91b4146",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T16:54:13.738968Z",
     "start_time": "2025-04-02T16:54:13.725929Z"
    }
   },
   "cell_type": "code",
   "source": "print(x_train[0])",
   "id": "6041f5a8a1a26158",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215686\n",
      " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
      " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313725\n",
      " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1372549  0.94509804\n",
      " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      " 0.58823529 0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.58039216\n",
      " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058824\n",
      " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
      " 0.31372549 0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333333 0.99215686\n",
      " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T13:23:22.049204Z",
     "start_time": "2025-04-02T13:23:22.038682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def to_one_hot(y, num_classes):\n",
    "    return np.eye(num_classes)[y]\n",
    "\n",
    "\n",
    "y_train = to_one_hot(y_train, 10)\n",
    "y_test = to_one_hot(y_test, 10)"
   ],
   "id": "8dd12b0de350a95",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T13:23:24.015629Z",
     "start_time": "2025-04-02T13:23:24.008673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weights = np.random.randn(in_features, out_features) * np.sqrt(2 / in_features)\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.dot(x, self.weights)\n",
    "\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(self.input.T, grad_output)\n",
    "        self.weights -= learning_rate * grad_weights\n",
    "        return grad_input"
   ],
   "id": "cb26b8a134d0cd7b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T13:23:24.491633Z",
     "start_time": "2025-04-02T13:23:24.480738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x > 0).astype(float)\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * self.mask"
   ],
   "id": "60ca03cb6e10518f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T13:25:27.940521Z",
     "start_time": "2025-04-02T13:25:27.931990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.output = np.tanh(x)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return (1 - self.output ** 2) * grad_output"
   ],
   "id": "16669880b1c3177a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T13:25:28.337651Z",
     "start_time": "2025-04-02T13:25:28.329924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, is_training=True):\n",
    "        if is_training:\n",
    "            self.mask = np.random.binomial(1, 1 - self.dropout_rate, size=x.shape)\n",
    "            return x * self.mask / (1 - self.dropout_rate)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * self.mask / (1 - self.dropout_rate)"
   ],
   "id": "9f1f1e124dfd372e",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T13:25:28.657750Z",
     "start_time": "2025-04-02T13:25:28.651792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        self.output = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_input = self.output * (grad_output - np.sum(self.output * grad_output, axis=1, keepdims=True))\n",
    "        return grad_input"
   ],
   "id": "9d045bc1e6d4ce2d",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T13:25:29.053416Z",
     "start_time": "2025-04-02T13:25:29.046280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import inspect\n",
    "\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x, is_training=True):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, is_training)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        for layer in reversed(self.layers):\n",
    "            if hasattr(layer, 'backward'):\n",
    "                if 'learning_rate' in inspect.signature(layer.backward).parameters:\n",
    "                    grad_output = layer.backward(grad_output, learning_rate)\n",
    "                else:\n",
    "                    grad_output = layer.backward(grad_output)"
   ],
   "id": "3b5bd1330a0dde2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T13:25:29.976521Z",
     "start_time": "2025-04-02T13:25:29.969758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "    grad = -(y_true / y_pred) / y_true.shape[0]\n",
    "    return loss, grad"
   ],
   "id": "8721573cecb13bcd",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T13:30:21.483335Z",
     "start_time": "2025-04-02T13:25:30.638598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Linear(in_features=784, out_features=512))\n",
    "model.add(Tanh())\n",
    "model.add(Dropout(dropout_rate=0.4))\n",
    "model.add(Linear(in_features=512, out_features=256))\n",
    "model.add(Tanh())\n",
    "model.add(Linear(in_features=256, out_features=10))\n",
    "model.add(Softmax())\n",
    "\n",
    "epochs = 25\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    correct_cnt = 0\n",
    "    for batch_start in range(0, len(x_train), batch_size):\n",
    "        batch_end = batch_start + batch_size\n",
    "        x_batch = x_train[batch_start:batch_end]\n",
    "        y_batch = y_train[batch_start:batch_end]\n",
    "\n",
    "        output = model.forward(x_batch, is_training=True)\n",
    "        loss, grad_loss = cross_entropy_loss(output, y_batch)\n",
    "        total_loss += loss\n",
    "\n",
    "        model.backward(grad_loss, learning_rate)\n",
    "        correct_cnt += np.sum(np.argmax(output, axis=1) == np.argmax(y_batch, axis=1))\n",
    "\n",
    "    test_output = model.forward(x_test, is_training=False)\n",
    "    test_accuracy = np.mean(np.argmax(test_output, axis=1) == y_test_labels)\n",
    "    train_accuracy = correct_cnt / len(x_train)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}, Train Acc: {train_accuracy:.4f}, Test Acc: {test_accuracy:.4f}\")"
   ],
   "id": "5c3eeda596778a41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 570.1766, Train Acc: 0.7928, Test Acc: 0.8967\n",
      "Epoch 2, Loss: 324.9849, Train Acc: 0.8819, Test Acc: 0.9093\n",
      "Epoch 3, Loss: 286.6279, Train Acc: 0.8952, Test Acc: 0.9173\n",
      "Epoch 4, Loss: 266.2610, Train Acc: 0.9026, Test Acc: 0.9221\n",
      "Epoch 5, Loss: 252.6877, Train Acc: 0.9076, Test Acc: 0.9249\n",
      "Epoch 6, Loss: 241.7728, Train Acc: 0.9114, Test Acc: 0.9274\n",
      "Epoch 7, Loss: 232.3119, Train Acc: 0.9158, Test Acc: 0.9301\n",
      "Epoch 8, Loss: 225.1838, Train Acc: 0.9182, Test Acc: 0.9324\n",
      "Epoch 9, Loss: 218.3729, Train Acc: 0.9200, Test Acc: 0.9346\n",
      "Epoch 10, Loss: 211.3081, Train Acc: 0.9226, Test Acc: 0.9376\n",
      "Epoch 11, Loss: 204.7622, Train Acc: 0.9258, Test Acc: 0.9398\n",
      "Epoch 12, Loss: 198.1061, Train Acc: 0.9275, Test Acc: 0.9411\n",
      "Epoch 13, Loss: 192.2500, Train Acc: 0.9302, Test Acc: 0.9430\n",
      "Epoch 14, Loss: 186.4229, Train Acc: 0.9315, Test Acc: 0.9440\n",
      "Epoch 15, Loss: 181.0100, Train Acc: 0.9346, Test Acc: 0.9451\n",
      "Epoch 16, Loss: 175.8722, Train Acc: 0.9356, Test Acc: 0.9468\n",
      "Epoch 17, Loss: 171.8333, Train Acc: 0.9377, Test Acc: 0.9489\n",
      "Epoch 18, Loss: 166.5427, Train Acc: 0.9397, Test Acc: 0.9502\n",
      "Epoch 19, Loss: 160.9492, Train Acc: 0.9410, Test Acc: 0.9517\n",
      "Epoch 20, Loss: 156.4017, Train Acc: 0.9429, Test Acc: 0.9529\n",
      "Epoch 21, Loss: 151.8258, Train Acc: 0.9446, Test Acc: 0.9544\n",
      "Epoch 22, Loss: 148.8206, Train Acc: 0.9455, Test Acc: 0.9553\n",
      "Epoch 23, Loss: 144.1826, Train Acc: 0.9474, Test Acc: 0.9565\n",
      "Epoch 24, Loss: 140.5619, Train Acc: 0.9494, Test Acc: 0.9577\n",
      "Epoch 25, Loss: 137.0517, Train Acc: 0.9497, Test Acc: 0.9583\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T13:59:52.699365Z",
     "start_time": "2025-04-02T13:59:50.895272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "weights_data = {\n",
    "    \"weight_1\": model.layers[0].weight,\n",
    "    \"weight_2\": model.layers[2].weight,\n",
    "    \"weight_3\": model.layers[4].weight,\n",
    "}\n",
    "\n",
    "with open(\"../backend/weights/weights.json\", 'w') as f:\n",
    "    json.dump(weights_data, f, indent=4)\n",
    "\n",
    "print(\"Веса успешно сохранены в файл\")"
   ],
   "id": "74b7c9ca693ed6fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 512)\n",
      "(512, 256)\n",
      "(256, 10)\n",
      "Веса успешно сохранены в файл\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-02T05:42:10.734293Z",
     "start_time": "2025-04-02T05:42:10.718720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(image_path, target_size=(28, 28)):\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    image = image.resize(target_size)\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    if np.mean(image_array) > 127:\n",
    "        image_array = 255 - image_array\n",
    "\n",
    "    image_array = image_array / 255.0\n",
    "\n",
    "    image_array = image_array.flatten().reshape(1, -1)\n",
    "\n",
    "    return image_array\n",
    "\n",
    "\n",
    "def predict_image(model, image_path):\n",
    "    processed_image = load_and_preprocess_image(image_path)\n",
    "    output = model.forward(processed_image, is_training=False)\n",
    "\n",
    "    probabilities = np.exp(output)\n",
    "\n",
    "    predicted_class = np.argmax(probabilities, axis=1)[0]\n",
    "\n",
    "    return predicted_class, probabilities\n",
    "\n",
    "\n",
    "image_path = r\"C:\\Users\\vgrig\\Desktop\\Untitled.png\"\n",
    "\n",
    "predicted_class, probabilities = predict_image(model, image_path)\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Probabilities: {probabilities}\")"
   ],
   "id": "6ccab859ef517338",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 8\n",
      "Probabilities: [[1.00746882 1.06759687 1.02625701 1.02844812 1.02859043 1.22769053\n",
      "  1.08010668 1.06062435 1.492448   1.10906811]]\n"
     ]
    }
   ],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
